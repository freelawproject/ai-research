{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b4c82f-4152-4834-b22a-e226317b6cb6",
   "metadata": {},
   "source": [
    "# Chunk the texts to context window sized chunks for embedding generation\n",
    "\n",
    "In order to generate embeddings for each case's content, we need to chunk the context to contex window sized chunks, generate embeddings for all chunks and then aggregate the embeddings to create one embedding per case. This notebook outlines the steps I undertook to chunk the content in preparation for embedding generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d7fb2-2189-4d46-bdee-fae4114ee786",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ffffcd1-ad12-4094-8170-1a57bc16febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nupunkt -q\n",
    "#%pip install transformers -q\n",
    "\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nupunkt import sent_tokenize\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b7665-c901-4cf3-8efd-4262fb468f1d",
   "metadata": {},
   "source": [
    "# Global Variables & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739014d4-eb78-4da6-af2a-70fe2c19edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "MAX_TOKENS = 8192\n",
    "MAX_SENTENCE = int(0.002 * MAX_TOKENS)\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80488dd2-56f4-4bea-87f9-dded0251f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, tokenizer=TOKENIZER, max_tokens=MAX_TOKENS, num_overlap_sentences=MAX_SENTENCE) -> list[str]:\n",
    "    \"\"\"Split text into chunks based on sentences, not exceeding max_tokens, with sentence overlap\"\"\"\n",
    "\n",
    "    # Split the text to sentences & encode sentences with tokenizer\n",
    "    sentences = sent_tokenize(text)\n",
    "    encoded_sentences = [\n",
    "        tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "    lead_text = \"\"\n",
    "    lead_tokens = tokenizer.encode(lead_text)\n",
    "    lead_len = len(lead_tokens)\n",
    "    chunks = []\n",
    "    current_chunks: list[str] = []\n",
    "    current_token_counts = len(lead_tokens)\n",
    "\n",
    "    for sentence_tokens in encoded_sentences:\n",
    "        sentence_len = len(sentence_tokens)\n",
    "        #print(sentence_len)\n",
    "        # if the current sentence itself is above max_tokens\n",
    "        if lead_len + sentence_len > max_tokens:\n",
    "            # store the previous chunk\n",
    "            if current_chunks:\n",
    "                chunks.append(lead_text + \" \".join(current_chunks))\n",
    "            # truncate the sentence and store the truncated sentence as its own chunk\n",
    "            truncated_sentence = tokenizer.decode(\n",
    "                sentence_tokens[: (max_tokens - len(lead_tokens))]\n",
    "            )\n",
    "            chunks.append(lead_text + truncated_sentence)\n",
    "\n",
    "            # start a new chunk with no overlap (because adding the current sentence will exceed the max_tokens)\n",
    "            current_chunks = []\n",
    "            current_token_counts = lead_len\n",
    "            continue\n",
    "\n",
    "        # if adding the new sentence will cause the chunk to exceed max_tokens\n",
    "        if current_token_counts + sentence_len > max_tokens:\n",
    "            overlap_sentences = current_chunks[\n",
    "                -max(0, num_overlap_sentences) :\n",
    "            ]\n",
    "            # store the previous chunk\n",
    "            if current_chunks:\n",
    "                chunks.append(lead_text + \" \".join(current_chunks))\n",
    "\n",
    "            overlap_token_counts = tokenizer.encode(\n",
    "                \" \".join(overlap_sentences), add_special_tokens=False\n",
    "            )\n",
    "            # If the sentence with the overlap exceeds the limit, start a new chunk without overlap.\n",
    "            if (\n",
    "                lead_len + len(overlap_token_counts) + sentence_len\n",
    "                > max_tokens\n",
    "            ):\n",
    "                current_chunks = [tokenizer.decode(sentence_tokens)]\n",
    "                current_token_counts = lead_len + sentence_len\n",
    "            else:\n",
    "                current_chunks = overlap_sentences + [\n",
    "                    tokenizer.decode(sentence_tokens)\n",
    "                ]\n",
    "                current_token_counts = (\n",
    "                    lead_len + len(overlap_token_counts) + sentence_len\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # if within max_tokens, continue to add the new sentence to the current chunk\n",
    "        current_chunks.append(tokenizer.decode(sentence_tokens))\n",
    "        current_token_counts += len(sentence_tokens)\n",
    "\n",
    "    # store the last chunk if it has any content\n",
    "    if current_chunks:\n",
    "        chunks.append(lead_text + \" \".join(current_chunks))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "515f90fb-0e28-4aa2-82fa-c005c3c7952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    'COVID-19': 'COVID',\n",
    "    'Benefits (Source)': 'BENEFITS',\n",
    "    'LGBTQ+': 'LGBTQ',\n",
    "    'Reproductive rights': 'REPRO',\n",
    "    'Policing': 'POLICING',\n",
    "    'Affected National Origin/Ethnicity(s)': 'NATION_ORIG',\n",
    "    'Voting': 'VOTE',\n",
    "    'Immigration/Border': 'IMMIGRATION',\n",
    "    'Medical/Mental Health Care': 'MED',\n",
    "    'Disability and Disability Rights': 'DISABILITY',\n",
    "    'Affected Race(s)': 'RACE',\n",
    "    'EEOC-centric': 'EEOC',\n",
    "    'Jails, Prisons, Detention Centers, and Other Institutions': 'PRISON',\n",
    "    'Affected Sex/Gender(s)': 'GENDER',\n",
    "    'Discrimination Area': 'DISC_AREA',\n",
    "    'Discrimination Basis': 'DISC_BASE',\n",
    "    'General/Misc.': 'GENERAL'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe8db0-025e-4afe-925d-84c519f8b710",
   "metadata": {},
   "source": [
    "# Process each file to create chunks of 8192 context window for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4948cd88-e27f-4421-a7f1-d2cb8319f098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  COVID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26739 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       9.070833\n",
       "std       14.054853\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        4.000000\n",
       "75%       10.250000\n",
       "max      132.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  BENEFITS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       6.762500\n",
       "std        9.008472\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        3.000000\n",
       "75%        8.000000\n",
       "max       64.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  LGBTQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       5.483333\n",
       "std        6.396238\n",
       "min        1.000000\n",
       "25%        1.750000\n",
       "50%        3.000000\n",
       "75%        7.000000\n",
       "max       58.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  REPRO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       6.491667\n",
       "std        9.222490\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        4.000000\n",
       "75%        7.000000\n",
       "max       96.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  POLICING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.0000\n",
       "mean       6.6125\n",
       "std        7.3641\n",
       "min        1.0000\n",
       "25%        1.0000\n",
       "50%        4.0000\n",
       "75%        8.2500\n",
       "max       44.0000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  NATION_ORIG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       7.829167\n",
       "std       16.062378\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        7.000000\n",
       "max      159.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  VOTE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       6.825000\n",
       "std       13.039151\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        7.000000\n",
       "max      159.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  IMMIGRATION\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       7.733333\n",
       "std       12.487807\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        4.000000\n",
       "75%        8.000000\n",
       "max      132.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  MED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       5.850000\n",
       "std        6.905398\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        8.000000\n",
       "max       46.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  DISABILITY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       7.004167\n",
       "std        9.325584\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        4.000000\n",
       "75%        8.000000\n",
       "max       68.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  RACE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       6.837500\n",
       "std       10.260348\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        9.000000\n",
       "max       92.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  EEOC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       4.362500\n",
       "std        7.483165\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        4.000000\n",
       "max       68.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  PRISON\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       5.616667\n",
       "std        7.169418\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        6.000000\n",
       "max       44.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  GENDER\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       4.650000\n",
       "std        7.427924\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        6.000000\n",
       "max       60.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  DISC_AREA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       7.045833\n",
       "std       11.242942\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        7.000000\n",
       "max       92.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  DISC_BASE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       5.900000\n",
       "std        8.266381\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        7.000000\n",
       "max       58.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  GENERAL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    240.000000\n",
       "mean       5.445833\n",
       "std        6.833739\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        7.000000\n",
       "max       48.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key, filename in abbreviations.items():\n",
    "    print(\"processing: \", filename)\n",
    "    df = pd.read_json(f'data/train/clean/{filename}.json')\n",
    "    \n",
    "    df['label'] = df['issue_category'].apply(lambda x: 1 if f'{key}' in x else 0)\n",
    "    df = df[[\"case_id\", \"content\", \"label\"]]\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        content = row[\"content\"].replace(\" .\", \"\").replace(\"=\", \"\").replace(\" - \", \"\")\n",
    "        chunks = split_text_into_chunks(content)\n",
    "        df.at[index, \"chunks\"] = f\"{chunks}\"\n",
    "        df.at[index, \"clean_content\"] = content\n",
    "        df.at[index, \"num_chunks\"] = len(chunks)\n",
    "    \n",
    "    df = df[[\"case_id\", \"clean_content\", \"chunks\", \"num_chunks\", \"label\"]]\n",
    "    \n",
    "    display(df[\"num_chunks\"].describe())\n",
    "\n",
    "    df.to_json(f'data/train/clean/chunks/{filename}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473ca61d-fd03-4aab-af53-312457e28bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  val\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    800.000000\n",
       "mean       5.400000\n",
       "std        8.283464\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        6.000000\n",
       "max      108.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"val\"\n",
    "print(\"processing: \", filename)\n",
    "df = pd.read_json(f'data/val/clean/{filename}.json')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    content = row[\"content\"].replace(\" .\", \"\").replace(\"=\", \"\").replace(\" - \", \"\")\n",
    "    chunks = split_text_into_chunks(content)\n",
    "    df.at[index, \"chunks\"] = f\"{chunks}\"\n",
    "    df.at[index, \"clean_content\"] = content\n",
    "    df.at[index, \"num_chunks\"] = len(chunks)\n",
    "\n",
    "df = df[[\"case_id\", \"clean_content\", \"chunks\", \"num_chunks\", \"issue_category\"]]\n",
    "\n",
    "display(df[\"num_chunks\"].describe())\n",
    "\n",
    "df.to_json(f'data/val/clean/chunks/{filename}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aafe965-c4ce-47a4-982c-f797f8eb0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    800.000000\n",
       "mean       5.820000\n",
       "std        9.515029\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        3.000000\n",
       "75%        6.000000\n",
       "max       99.000000\n",
       "Name: num_chunks, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = \"test\"\n",
    "print(\"processing: \", filename)\n",
    "df = pd.read_json(f'data/test/clean/{filename}.json')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    content = row[\"content\"].replace(\" .\", \"\").replace(\"=\", \"\").replace(\" - \", \"\")\n",
    "    chunks = split_text_into_chunks(content)\n",
    "    df.at[index, \"chunks\"] = f\"{chunks}\"\n",
    "    df.at[index, \"clean_content\"] = content\n",
    "    df.at[index, \"num_chunks\"] = len(chunks)\n",
    "\n",
    "df = df[[\"case_id\", \"clean_content\", \"chunks\", \"num_chunks\", \"issue_category\"]]\n",
    "\n",
    "display(df[\"num_chunks\"].describe())\n",
    "\n",
    "df.to_json(f'data/test/clean/chunks/{filename}.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
